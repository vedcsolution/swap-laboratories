groups:
    big-ones:
        exclusive: true
        members:
            - MiniMax-M2.5-AWQ
            - Qwen/Qwen3-Coder-Next-FP8
            - gpt-oss-120b
        swap: true
    managed-recipes:
        exclusive: true
        members: []
        swap: true
    medium:
        exclusive: true
        members:
            - GLM-4.7-Flash-AWQ-4bit
            - Nemotron-3-Nano-NVFP4
        swap: true
healthCheckTimeout: 600
hooks: null
macros:
    llama_root: /home/csolutions_ai/Swap-Laboratories
    recipe_runner: /home/csolutions_ai/spark-trtllm-docker/run-recipe.sh
    spark_root: /home/csolutions_ai/spark-trtllm-docker
    trtllm_container: trtllm-multinode
    trtllm_head_ip: 192.168.200.12
    trtllm_nodes: 192.168.200.12,192.168.200.13
    trtllm_stop_cluster: |-
        for n in $(echo "${trtllm_nodes}" | tr "," " "); do
          n="$(echo "$n" | xargs)";
          if [ -z "$n" ]; then continue; fi;
          if [ "$n" = "${trtllm_head_ip}" ]; then continue; fi;
          ssh -o BatchMode=yes -o ConnectTimeout=5 -o StrictHostKeyChecking=no "$n" "docker rm -f ${trtllm_container} >/dev/null 2>&1 || true" || true;
        done; docker rm -f ${trtllm_container} >/dev/null 2>&1 || true
    user_home: ${env.HOME}
    vllm_container: vllm_node
    vllm_exec_env: -e VLLM_HOST_IP=${vllm_head_ip} -e RAY_NODE_IP_ADDRESS=${vllm_head_ip} -e RAY_OVERRIDE_NODE_IP_ADDRESS=${vllm_head_ip} -e NCCL_SOCKET_IFNAME=${vllm_iface} -e GLOO_SOCKET_IFNAME=${vllm_iface} -e TP_SOCKET_IFNAME=${vllm_iface}
    vllm_head_ip: 192.168.200.12
    vllm_iface: enp1s0f1np1
    vllm_marlin_image: vllm-node-marlin-sm12x
    vllm_nodes: 192.168.200.12,192.168.200.13
    vllm_stop_cluster: |-
        for n in $(echo "${trtllm_nodes}" | tr "," " "); do
          n="$(echo "$n" | xargs)";
          if [ -z "$n" ]; then continue; fi;
          if [ "$n" = "${vllm_head_ip}" ]; then continue; fi;
          ssh -o BatchMode=yes -o ConnectTimeout=5 -o StrictHostKeyChecking=no "$n" "docker rm -f ${vllm_container} >/dev/null 2>&1 || true" || true;
        done; docker rm -f ${vllm_container} >/dev/null 2>&1 || true
models:
    GLM-4.7-Flash-AWQ-4bit:
        aliases:
            - cyankiwi/GLM-4.7-Flash-AWQ-4bit
            - glm-4.7-flash
        checkEndpoint: /health
        cmd: bash -lc '${trtllm_stop_cluster}; exec ${recipe_runner} glm-4.7-flash-awq -n ${trtllm_nodes} --tp 2 --port ${PORT}'
        cmdStop: |
            bash -lc '${trtllm_stop_cluster}'
        description: TRT-LLM GLM-4.7-Flash-AWQ
        name: GLM-4.7-Flash-AWQ-4bit
        proxy: http://127.0.0.1:${PORT}
        ttl: 0
        useModelName: glm-4.7-flash
    MiniMax-M2.5-AWQ:
        aliases:
            - MiniMax-M2.5-AWQ
            - cyankiwi/MiniMax-M2.5-AWQ-4bit
            - cyankiwi/MiniMax-M2.1-AWQ-4bit
        checkEndpoint: /health
        cmd: bash -lc '${trtllm_stop_cluster}; exec ${recipe_runner} minimax-m2.1-awq-4bit -n ${trtllm_nodes} --tp 2 --port ${PORT}'
        cmdStop: |
            bash -lc '${trtllm_stop_cluster}'
        description: TRT-LLM MiniMax M2.1 AWQ 4bit
        name: MiniMax-M2.5-AWQ
        proxy: http://127.0.0.1:${PORT}
        ttl: 0
        useModelName: cyankiwi/MiniMax-M2.1-AWQ-4bit
    Nemotron-3-Nano-NVFP4:
        aliases:
            - nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4-nano
        checkEndpoint: /health
        cmd: bash -lc '${trtllm_stop_cluster}; exec ${recipe_runner} nemotron-3-nano-nvfp4 --solo --port ${PORT}'
        cmdStop: |
            bash -lc '${trtllm_stop_cluster}'
        description: TRT-LLM Nemotron nano recipe
        metadata:
            benchy:
                trust_remote_code: true
        name: Nemotron-3-Nano-NVFP4 (solo recipe)
        proxy: http://127.0.0.1:${PORT}
        ttl: 0
        useModelName: nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4
    Qwen/Qwen3-Coder-Next-FP8:
        aliases:
            - Qwen/Qwen3-Coder-Next-FP8
        checkEndpoint: /health
        cmd: bash -lc '${trtllm_stop_cluster}; exec ${recipe_runner} qwen3-coder-next-fp8 -n ${trtllm_nodes} --tp 2 --port ${PORT}'
        cmdStop: bash -lc '${trtllm_stop_cluster}'
        description: TRT-LLM Qwen3 NEXT FP8
        metadata:
            benchy:
                trust_remote_code: true
            recipe_ui:
                backend_dir: /home/csolutions_ai/spark-trtllm-docker
                group: big-ones
                managed: true
                mode: cluster
                nodes: ${trtllm_nodes}
                recipe_ref: qwen3-coder-next-fp8
                tensor_parallel: 2
        name: Qwen/Qwen3-Coder-Next-FP8
        proxy: http://127.0.0.1:${PORT}
        ttl: 0
        unlisted: false
        useModelName: Qwen/Qwen3-Coder-Next-FP8
    gpt-oss-20b:
        aliases:
            - gpt-oss-20b
            - openai/gpt-oss-20b
        checkEndpoint: /health
        cmd: bash -lc 'exec ${recipe_runner} openai-gpt-oss-20b --solo --tp 1 --port ${PORT} --max-num-tokens 1024 --max-batch-size 1 --model-dir /home/csolutions_ai/.cache/huggingface/hub/models--openai--gpt-oss-20b'
        cmdStop: |
            bash -lc 'docker rm -f trtllm-multinode >/dev/null 2>&1 || true'
        description: TRT-LLM AutoDeploy GPT-OSS 20B (solo)
        metadata:
            benchy:
                trust_remote_code: false
            recipe_ui:
                backend_dir: /home/csolutions_ai/spark-trtllm-docker
                extra_args: --max-num-tokens 1024 --max-batch-size 1 --model-dir /home/csolutions_ai/.cache/huggingface/hub/models--openai--gpt-oss-20b
                managed: true
                mode: solo
                recipe_ref: openai-gpt-oss-20b
                tensor_parallel: 1
        name: GPT-OSS 20B
        proxy: http://127.0.0.1:${PORT}
        ttl: 0
        useModelName: openai/gpt-oss-20b
    gpt-oss-120b:
        aliases:
            - gpt-oss-120b
            - openai/gpt-oss-120b
        checkEndpoint: /health
        cmd: bash -lc '${trtllm_stop_cluster}; exec ${recipe_runner} openai-gpt-oss-120b -n ${trtllm_nodes} --tp 2 --port ${PORT}'
        cmdStop: |
            bash -lc '${trtllm_stop_cluster}'
        description: TRT-LLM GPT-OSS 120B
        metadata:
            benchy:
                trust_remote_code: false
            recipe_ui:
                backend_dir: /home/csolutions_ai/spark-trtllm-docker
                group: big-ones
                managed: true
                mode: cluster
                nodes: ${trtllm_nodes}
                recipe_ref: openai-gpt-oss-120b
                tensor_parallel: 2
        name: GPT-OSS 120B
        proxy: http://127.0.0.1:${PORT}
        ttl: 0
        useModelName: openai/gpt-oss-120b
startPort: 8089
